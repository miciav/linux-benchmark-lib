- name: Select workload runner mode
  debug:
    msg: >-
      Workload runner mode={{ workload_runner_mode | default('execute') }}
      tests={{ workload_runner_tests | default([]) }}
      output={{ workload_runner_output_dir | default('/tmp') }}

- name: Define workload runner workdir
  set_fact:
    workload_runner_workdir: "{{ workload_runner_workdir | default('/tmp/linux-benchmark-lib') }}"

- name: Create workload runner workdir
  file:
    path: "{{ workload_runner_workdir }}"
    state: directory
    mode: "0755"

- name: Create virtual environment for workload runner
  ansible.builtin.command:
    cmd: "python3 -m venv {{ workload_runner_workdir }}/.venv"
    creates: "{{ workload_runner_workdir }}/.venv/bin/python"
  when:
    - workload_runner_mode == "execute"
    - workload_runner_install_deps | default(false)

- name: Synchronize project to remote host
  ansible.builtin.copy:
    src: "{{ playbook_dir }}/../../{{ item }}"
    dest: "{{ workload_runner_workdir }}/{{ item }}"
    mode: preserve
  loop:
    - benchmark_config.py
    - local_runner.py
    - data_handler.py
    - reporter.py
    - metric_collectors
    - workload_generators
    - pyproject.toml
    - uv.lock
  when: workload_runner_mode == "execute"

- name: Ensure workload output directory exists
  file:
    path: "{{ workload_runner_output_dir | default('/tmp') }}"
    state: directory
    mode: "0755"

- name: Prepare benchmark configuration for this host
  set_fact:
    workload_runner_config_rendered: >-
      {{
        (workload_runner_config | default({}))
        | combine({
            'output_dir': workload_runner_output_dir | default('/tmp'),
            'report_dir': (workload_runner_output_dir | default('/tmp')) ~ '/reports',
            'data_export_dir': (workload_runner_output_dir | default('/tmp')) ~ '/exports',
          }, recursive=True)
      }}

- name: Write benchmark configuration file
  copy:
    dest: "{{ workload_runner_workdir }}/benchmark_config.generated.json"
    content: "{{ workload_runner_config_rendered | to_nice_json }}"

- name: Upgrade pip inside workload runner venv
  ansible.builtin.command:
    cmd: "{{ workload_runner_workdir }}/.venv/bin/pip install --upgrade pip"
    chdir: "{{ workload_runner_workdir }}"
  when:
    - workload_runner_mode == "execute"
    - workload_runner_install_deps | default(false)

- name: Install project in editable mode (explicit)
  ansible.builtin.command:
    cmd: "{{ workload_runner_workdir }}/.venv/bin/pip install -e ."
    chdir: "{{ workload_runner_workdir }}"
  when:
    - workload_runner_mode == "execute"
    - workload_runner_install_deps | default(false)

- name: Run benchmark via local runner
  ansible.builtin.shell: |
    {{ workload_runner_workdir }}/.venv/bin/python - <<'PY' | tee -a "{{ workload_runner_output_dir | default('/tmp') }}/runner.log"
    from pathlib import Path
    import sys
    import logging

    # Configure logging to ensure output is captured
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        stream=sys.stdout
    )

    # __file__ is not defined in stdin scripts; use cwd provided by Ansible
    workdir = Path.cwd().resolve()
    sys.path.insert(0, str(workdir))
    sys.path.insert(0, str(workdir / "metric_collectors"))
    sys.path.insert(0, str(workdir / "workload_generators"))
    from benchmark_config import BenchmarkConfig
    from local_runner import LocalRunner

    config_path = Path("benchmark_config.generated.json")
    with config_path.open() as fh:
        cfg = BenchmarkConfig.from_json(fh.read())

    runner = LocalRunner(cfg)
    test_name = "{{ item }}"
    print(f"Running benchmark: {test_name}", flush=True)
    
    print(f"\n>>> Starting {test_name}", flush=True)
    runner.run_benchmark(test_name)
    print(f"<<< Finished {test_name}\n", flush=True)
    PY
  args:
    chdir: "{{ workload_runner_workdir }}"
  environment:
    PYTHONPATH: "{{ workload_runner_workdir }}:{{ lookup('env', 'PYTHONPATH') | default('') }}"
    PYTHONUNBUFFERED: "1"
  loop: "{{ workload_runner_tests | default([]) }}"
  loop_control:
    label: "{{ item }}"
  when:
    - workload_runner_mode == "execute"
