{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>A robust, configurable Python library for benchmarking Linux compute nodes.</p>"},{"location":"#overview","title":"Overview","text":"<p>Run repeatable workloads and collect detailed system metrics under synthetic load. The library helps you understand performance variability across repetitions and workloads. It supports two operation modes: 1. Agent/Runner: Lightweight installation for target nodes (local execution). 2. Controller: Full orchestration layer for managing remote benchmarks (includes Ansible, plotting tools). 3. UI/CLI: User interaction lives in <code>lb_ui</code> and talks only to the controller; the runner does not import or know about UI concerns.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Multi-level metrics: PSUtil, Linux CLI tools, perf events, optional eBPF</li> <li>Plugin workloads: stress-ng, dd, fio, and HPL shipped as plugins, extensible via entry points</li> <li>Data aggregation: Pandas DataFrames with metrics as index, repetitions as columns</li> <li>Reporting: Text reports and plots (Controller only)</li> <li>Centralized config: Typed dataclasses for all knobs</li> <li>Remote execution: Python controller + Ansible Runner targeting remote hosts or <code>localhost</code></li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.13+</li> <li>Linux for full functionality</li> <li>Root privileges for some features (perf, eBPF)</li> </ul>"},{"location":"#required-external-software-target-nodes","title":"Required External Software (Target Nodes)","text":"<ul> <li>sysstat: sar, vmstat, iostat, mpstat, pidstat</li> <li>stress-ng: load generator</li> <li>fio: advanced I/O testing</li> <li>HPL: Linpack benchmark (optional)</li> <li>perf: Linux profiling</li> <li>bcc/eBPF tools: optional kernel-level metrics</li> </ul>"},{"location":"#installation","title":"Installation","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone &lt;repository-url&gt;\ncd linux-benchmark-lib\n</code></pre>"},{"location":"#mode-1-agent-lightweight","title":"Mode 1: Agent (Lightweight)","text":"<p>Installs only the core dependencies for running benchmarks on a target node.</p> <pre><code>uv sync\n# Or install as a tool\nuv tool install .\n</code></pre>"},{"location":"#mode-2-controller-full","title":"Mode 2: Controller (Full)","text":"<p>Installs the core plus orchestration tools (Ansible) and reporting libraries (Matplotlib, Seaborn).</p> <pre><code>uv sync --extra controller\n# Or install as a tool\nuv tool install \".[controller]\"\n</code></pre>"},{"location":"#development","title":"Development","text":"<p>Installs all dependencies including test and linting tools.</p> <pre><code>uv sync --all-extras --dev\n</code></pre> <p>Switch between modes quickly with the helper script:</p> <pre><code>bash tools/switch_mode.sh base        # core only\nbash tools/switch_mode.sh controller  # adds controller extra\nbash tools/switch_mode.sh dev         # dev + all extras\n</code></pre>"},{"location":"#cli-lb","title":"CLI (lb)","text":"<p>See <code>CLI.md</code> for the full command reference. Highlights: - Config and defaults: <code>lb config init</code>, <code>lb config set-default</code>, <code>lb config edit</code>, <code>lb config workloads</code>, <code>lb plugin list --select/--enable/--disable NAME</code> (shows enabled state with checkmarks). - Discovery and run: <code>lb plugin list</code>, <code>lb hosts</code>, <code>lb run [tests...]</code> (follows config for local/remote unless overridden). - Interactive toggle: <code>lb plugin select</code> to enable/disable plugins with arrows + space; <code>lb config select-workloads</code> to toggle configured workloads the same way. - Install plugins from a path or git repo: <code>lb plugin install /path/to/sysbench_plugin.tar.gz</code> or <code>lb plugin install https://github.com/miciav/unixbench-lb-plugin.git</code>. - Third-party plugins are installed under <code>lb_runner/plugins/_user</code> when that directory is writable (e.g., in a repo checkout or remote runner tree), so moving the runner also moves installed plugins. If not writable, the installer falls back to <code>~/.config/lb/plugins</code>. You can override the location with <code>LB_USER_PLUGIN_DIR</code>. - Example (UnixBench from git):    <code>bash   lb plugin install https://github.com/miciav/unixbench-lb-plugin.git   lb plugin list --enable unixbench</code>   Then set options in <code>benchmark_config.json</code> if needed:   <code>json   \"workloads\": {\"unixbench\": {\"plugin\": \"unixbench\", \"enabled\": true, \"options\": {\"concurrency\": 4}}}</code> - Health checks: <code>lb doctor controller</code>, <code>lb doctor local-tools</code>, <code>lb doctor multipass</code>, <code>lb doctor all</code>. - Integration helper: <code>lb test multipass --vm-count {1,2} [--multi-workloads]</code> (artifacts to <code>tests/results</code> by default). - Test helpers (<code>lb test ...</code>) are available in dev mode (create <code>.lb_dev_cli</code> or export <code>LB_ENABLE_TEST_CLI=1</code>).</p>"},{"location":"#ui-layer","title":"UI layer","text":"<ul> <li>The CLI/UI entrypoint is <code>python -m lb_ui.cli</code> (or the installed <code>lb</code> shim). Runner/controller modules no longer import UI.</li> <li>Progress bars and tables are text-friendly; headless output works in CI and when piping.</li> <li>Force headless output with <code>LB_HEADLESS_UI=1</code> when running under CI or when piping output.</li> <li>UI adapters live in <code>lb_ui/ui/*</code> and depend on controller-owned interfaces (<code>lb_controller.ui_interfaces</code>); the runner only emits events/logs.</li> </ul>"},{"location":"#plugin-manifests-and-generated-assets","title":"Plugin manifests and generated assets","text":"<ul> <li>Each workload is self-contained in <code>lb_runner/plugins/&lt;name&gt;/</code>.</li> <li>Dependencies are defined in the plugin's Python class (<code>get_required_apt_packages</code>, etc.).</li> <li>A dedicated Dockerfile can be provided in the plugin directory for containerized execution.</li> </ul> <p>This updates the generated apt/pip install block in <code>Dockerfile</code> and rewrites <code>lb_controller/ansible/roles/workload_runner/tasks/plugins.generated.yml</code>. - Commit both the manifest and generated files so remote setup and the container stay in sync with available plugins. - See <code>docs/PLUGIN_DEVELOPMENT.md</code> for a full plugin authoring guide (WorkloadPlugin interface, manifests, packaging, git installs). - HPL plugin: vedi <code>lb_runner/plugins/hpl/README.md</code> per note su packaging <code>.deb</code>, build VM/Docker e test <code>xhpl</code>.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from lb_runner.benchmark_config import BenchmarkConfig, RemoteHostConfig, RemoteExecutionConfig\nfrom lb_controller.controller import BenchmarkController\nfrom lb_runner.local_runner import LocalRunner\nfrom lb_runner.plugin_system.builtin import builtin_plugins\nfrom lb_runner.plugin_system.registry import PluginRegistry\n\n# Create a configuration\nconfig = BenchmarkConfig(\n    repetitions=3,\n    test_duration_seconds=3600,\n    metrics_interval_seconds=1.0\n)\n\n# Local execution (Agent Mode)\nregistry = PluginRegistry(builtin_plugins())\nrunner = LocalRunner(config, registry=registry)\nrunner.run_benchmark(\"stress_ng\")\n\n# Remote execution (Controller Mode)\n# Requires 'controller' extra installed\nremote_config = BenchmarkConfig(\n    remote_hosts=[RemoteHostConfig(name=\"node1\", address=\"192.168.1.10\", user=\"ubuntu\")],\n    remote_execution=RemoteExecutionConfig(enabled=True),\n)\n# Use distinct, non-empty `name` values per host; they become per-host output dirs.\ncontroller = BenchmarkController(remote_config)\nsummary = controller.run([\"stress_ng\"], run_id=\"demo-run\")\nprint(summary.per_host_output)\n</code></pre>"},{"location":"#project-layout","title":"Project Layout","text":"<pre><code>linux-benchmark-lib/\n\u251c\u2500\u2500 lb_runner/           # Runner (plugins, collectors, local runner, events)\n\u251c\u2500\u2500 lb_controller/       # Orchestration (services, ansible, journal, data_handler)\n\u251c\u2500\u2500 lb_ui/               # CLI/UI (Typer app, adapters, reporter)\n\u251c\u2500\u2500 tests/               # Unit and integration tests\n\u251c\u2500\u2500 tools/               # Helper scripts (mode switching, etc.)\n\u2514\u2500\u2500 pyproject.toml       # Project configuration (Core + Extras)\n</code></pre>"},{"location":"#configuration","title":"Configuration","text":"<p>All knobs are defined in <code>BenchmarkConfig</code>:</p> <pre><code>from pathlib import Path\nfrom lb_runner.benchmark_config import BenchmarkConfig\nfrom lb_runner.plugins.stress_ng.plugin import StressNGConfig\n\nconfig = BenchmarkConfig(\n    repetitions=5,\n    test_duration_seconds=120,\n    metrics_interval_seconds=0.5,\n    plugin_settings={\n        \"stress_ng\": StressNGConfig(\n            cpu_workers=4,\n            vm_workers=2,\n            vm_bytes=\"2G\",\n        )\n    },\n)\n\nconfig.save(Path(\"my_config.json\"))\nconfig = BenchmarkConfig.load(Path(\"my_config.json\"))\n</code></pre>"},{"location":"#output","title":"Output","text":"<p>Results are written to three directories:</p> <ul> <li><code>benchmark_results/</code>: raw metric data</li> <li><code>reports/</code>: text reports and plots (via analytics)</li> <li><code>data_exports/</code>: aggregated CSV/JSON (generated on demand via <code>lb analyze</code>)</li> <li>In remote mode results are split by <code>run_id/host</code> (e.g., <code>benchmark_results/run-YYYYmmdd-HHMMSS/node1/...</code>).</li> </ul>"},{"location":"#diagrams-uml","title":"Diagrams (UML)","text":"<p>The repository ships an action that generates UML class/package diagrams on each release. To regenerate them locally (requires Graphviz installed):</p> <pre><code>pip install \"pylint==3.3.1\"\nmkdir -p docs/diagrams\npyreverse -o png -p linux-benchmark lb_runner lb_controller lb_ui -S\nmv classes*.png docs/diagrams/classes.png\nmv packages*.png docs/diagrams/packages.png\npyreverse -o puml -p linux-benchmark lb_runner lb_controller lb_ui -S\nmv classes*.puml docs/diagrams/classes.puml\nmv packages*.puml docs/diagrams/packages.puml\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<ol> <li>Fork the project</li> <li>Create a feature branch (<code>git checkout -b feature/AmazingFeature</code>)</li> <li>Commit your changes (<code>git commit -m 'Add some AmazingFeature'</code>)</li> <li>Push the branch (<code>git push origin feature/AmazingFeature</code>)</li> <li>Open a Pull Request</li> </ol>"},{"location":"#licenza","title":"Licenza","text":"<p>Distribuito sotto licenza MIT. Vedi <code>LICENSE</code> per maggiori informazioni.</p>"},{"location":"cli/","title":"CLI Reference","text":"<p>Note: the user-facing CLI lives in <code>lb_ui</code> (invoke via <code>lb</code> or <code>python -m lb_ui.cli</code>). Runner/controller packages no longer import the UI or expose separate entrypoints.</p>"},{"location":"cli/#setup","title":"Setup","text":"<ul> <li>Install in a venv: <code>uv venv &amp;&amp; uv pip install -e .</code></li> <li>Make <code>lb</code> globally available (optional): <code>uv tool install -e .</code></li> <li>Enable shell completion: <code>lb --install-completion</code> (bash/zsh/fish) and restart your shell.</li> </ul>"},{"location":"cli/#config-resolution","title":"Config resolution","text":"<p>Order used by commands that need a config: 1. <code>-c/--config</code> flag 2. Saved default at <code>~/.config/lb/config_path</code> (set via <code>lb config set-default</code> or <code>lb config init</code>) 3. <code>./benchmark_config.json</code> if present 4. Built-in defaults</p>"},{"location":"cli/#top-level-commands","title":"Top-level commands","text":"<ul> <li><code>lb plugin list|ls [--select] [--enable NAME | --disable NAME] [-c FILE] [--set-default]</code>   Show plugins with enabled state; optionally enable/disable a workload in the config or open an interactive selector (arrows + space). (<code>lb plugins</code> remains as a compatibility alias.)</li> <li><code>lb plugin select [-c FILE] [--set-default]</code>   Directly open the interactive selector to toggle plugins with arrows + space.</li> <li><code>lb plugin install PATH|URL [--manifest FILE] [--force] [--regen-assets/--no-regen-assets]</code>   Install a plugin from a .py file, directory, archive (.zip/.tar.gz), or git repository URL.</li> <li><code>lb plugin uninstall NAME [--purge-config/--keep-config] [--regen-assets/--no-regen-assets]</code>   Remove a user plugin and optionally delete its config entries.</li> <li><code>lb hosts [-c FILE]</code>   Show remote hosts from the resolved config.</li> <li><code>lb run [TEST ...] [-c FILE] [--run-id ID] [--remote/--no-remote] [--repetitions N]</code>   Run workloads locally or remotely (auto-follows config unless overridden). Use <code>--repetitions</code> to temporarily change how many times each workload runs.</li> <li><code>lb run ... --docker [--docker-image TAG] [--docker-engine docker|podman] [--docker-no-build] [--docker-no-cache]</code>   Build/use the container image and run the CLI inside it. Mounts the repo read-only and writes artifacts to the container\u2019s <code>benchmark_results</code>.</li> <li><code>lb runs list [--root PATH] [-c FILE]</code> / <code>lb runs show RUN_ID [--root PATH] [-c FILE]</code>   List past runs stored under <code>benchmark_results/</code> and inspect a single run (hosts, workloads, paths).</li> <li><code>lb analyze [RUN_ID] [--root PATH] [--kind aggregate] [--workload NAME] [--host NAME]</code>   Run post-processing analytics on an existing run. If <code>RUN_ID</code> or selectors are omitted and you are in an interactive TTY, prompts will guide selection.</li> </ul>"},{"location":"cli/#config-management-lb-config","title":"Config management (<code>lb config ...</code>)","text":"<ul> <li><code>lb config init [-i] [--path FILE] [--set-default/--no-set-default]</code>   Create a config (prompt for a remote host with <code>-i</code>).</li> <li><code>lb config set-repetitions N [-c FILE] [--set-default/--no-set-default]</code>   Persist the desired number of repetitions to a config file (defaults to <code>~/.config/lb/config.json</code>).</li> <li><code>lb config set-default FILE</code> / <code>lb config unset-default</code> / <code>lb config show-default</code></li> <li><code>lb config edit [-p FILE]</code>   Open the config in <code>$EDITOR</code>.</li> <li><code>lb config workloads [-c FILE]</code>   List workloads and enabled status.</li> <li><code>lb config enable-workload NAME [-c FILE] [--set-default]</code>   (creates if missing)</li> <li><code>lb config disable-workload NAME [-c FILE] [--set-default]</code></li> </ul>"},{"location":"cli/#doctor-checks-lb-doctor","title":"Doctor checks (<code>lb doctor ...</code>)","text":"<ul> <li><code>lb doctor controller</code> \u2014 Python deps + ansible/ansible-runner + config resolution.</li> <li><code>lb doctor local-tools</code> \u2014 stress-ng, fio, sysstat tools, perf (needed only for local runs).</li> <li><code>lb doctor multipass</code> \u2014 check presence of multipass (optional).</li> <li><code>lb doctor all</code> \u2014 run all checks.</li> </ul>"},{"location":"cli/#integration-helper-lb-test-dev-installs-only","title":"Integration helper (<code>lb test ...</code>, dev installs only)","text":"<ul> <li>Available when <code>.lb_dev_cli</code> exists in the project root or <code>LB_ENABLE_TEST_CLI=1</code> is set.</li> <li><code>lb test multipass [-o DIR] [--vm-count {1,2}] [--multi-workloads] [-- EXTRA_PYTEST_ARGS...]</code>   Runs the Multipass integration test. Artifacts go to <code>tests/results</code> by default (override with <code>-o</code> or <code>LB_TEST_RESULTS_DIR</code>). <code>--vm-count</code> (or <code>LB_MULTIPASS_VM_COUNT</code>) launches 1\u20132 VMs. Use <code>--multi-workloads</code> to run the stress_ng + dd + fio variant. Requires multipass + ansible/ansible-runner locally.</li> </ul>"}]}