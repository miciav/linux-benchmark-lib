...sss..F
=================================== FAILURES ===================================
______________________ test_dfaas_multipass_cli_workflow _______________________

multipass_two_vms = [{'ip': '192.168.2.2', 'key_path': '/Users/micheleciavotta/Downloads/linux-benchmark-lib/temp_keys/dfaas_multipass_key...eleciavotta/Downloads/linux-benchmark-lib/temp_keys/dfaas_multipass_key', 'name': 'dfaas-generator', 'user': 'ubuntu'}]
tmp_path = PosixPath('/private/var/folders/q1/5ln0gtyj3fv55055frjp0nx80000gn/T/pytest-of-micheleciavotta/pytest-374/test_dfaas_multipass_cli_workf0')

    def test_dfaas_multipass_cli_workflow(multipass_two_vms, tmp_path: Path) -> None:
        """E2E test that executes DFaaS benchmark via CLI (lb run --remote).
    
        This test verifies the complete DFaaS workflow as described in the prompt:
        1. Infrastructure Setup: Two VMs (target + generator) with SSH access
        2. Configuration Generation: Dynamic JSON config file
        3. Benchmark Execution: Via `uv run lb run --remote -c <config>`
        4. Verification: Artifacts, results content, k6 logs, and LB_EVENT output
    
        Uses minimal test config: rate=10, duration=10s, single function.
        """
        _ensure_local_prereqs()
        target_vm, k6_vm = multipass_two_vms[0], multipass_two_vms[1]
        k6_workspace_root = _k6_workspace_root(k6_vm["user"])
    
        ansible_dir = tmp_path / "ansible_cli"
        staged_key = stage_private_key(Path(target_vm["key_path"]),
                                       ansible_dir / "keys")
        target_host = {
            "ip": target_vm["ip"],
            "user": target_vm["user"],
            "key": str(staged_key),
        }
        k6_host = {
            "ip": k6_vm["ip"],
            "user": k6_vm["user"],
            "key": str(staged_key),
        }
    
        ansible_env = make_test_ansible_env(ansible_dir)
        target_inventory = ansible_dir / "target_inventory.ini"
        k6_inventory = ansible_dir / "k6_inventory.ini"
        _write_inventory(target_inventory, target_host)
        _write_inventory(k6_inventory, k6_host)
    
        # Setup target (k6 provisioning is executed from target)
        setup_target = Path("lb_plugins/plugins/dfaas/ansible/setup_target.yml")
        setup_k6 = Path("lb_plugins/plugins/dfaas/ansible/setup_k6.yml")
    
        # Use fixed /tmp path for output to avoid cross-platform path issues (macOS -> Linux VM)
        output_dir = Path("/tmp/lb_e2e_results")
        if output_dir.exists():
            shutil.rmtree(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        lb_workdir = _lb_workdir(target_vm["user"])
    
        report_dir = tmp_path / "reports"
        export_dir = tmp_path / "exports"
    
        k6_key_target_path = f"/home/{target_vm['user']}/.ssh/dfaas_k6_key"
        k6_playbook_target_path = "/tmp/setup_k6.yml"
        try:
            _copy_private_key_to_target(
                target_vm["name"], staged_key, k6_key_target_path
            )
            _copy_file_to_target(
                target_vm["name"],
                setup_k6,
                k6_playbook_target_path,
            )
        except Exception as exc:  # noqa: BLE001
            _skip_or_fail("Failed to copy k6 SSH key to target", context="copy_k6_key_cli", exc=exc)
    
        try:
            _run_playbook(
                setup_target,
                target_inventory,
                {
                    "openfaas_functions": ["env"],
                    "k6_host": k6_vm["ip"],
                    "k6_user": k6_vm["user"],
                    "k6_ssh_key": k6_key_target_path,
                    "k6_port": 22,
                    "k6_workspace_root": k6_workspace_root,
                    "k6_setup_playbook_path": k6_playbook_target_path,
                },
                ansible_env,
            )
        except Exception as exc:  # noqa: BLE001
            _skip_or_fail("setup_target playbook failed", context="setup_target_cli", exc=exc)
    
        try:
            password = _get_openfaas_password(target_vm["name"])
        except Exception as exc:  # noqa: BLE001
            _skip_or_fail("Failed to read OpenFaaS password", context="openfaas_password_cli", exc=exc)
    
        auth_value = base64.b64encode(f"admin:{password}".encode("utf-8")).decode("utf-8")
    
        # DFaaS plugin options - used in both plugin_settings and workloads.options
        dfaas_options = {
            "gateway_url": f"http://{target_vm['ip']}:31112",
            "prometheus_url": "http://127.0.0.1:30411",
            "k6_host": k6_vm["ip"],
            "k6_user": k6_vm["user"],
            "k6_ssh_key": k6_key_target_path,
            "k6_port": 22,
            "k6_workspace_root": k6_workspace_root,
            "k6_log_stream": True,
            "functions": [
                {
                    "name": "env",
                    "method": "GET",
                    "body": "",
                    "headers": {"Authorization": f"Basic {auth_value}"},
                }
            ],
            "rates": {"min_rate": 10, "max_rate": 10, "step": 10},
            "combinations": {"min_functions": 1, "max_functions": 2},
            "duration": "30s",
            "iterations": 1,
            "cooldown": {
                "max_wait_seconds": 60,
                "sleep_step_seconds": 5,
                "idle_threshold_pct": 20,
            },
        }
    
        target_vm_name = target_vm["name"]
    
        config = {
            "repetitions": 1,
            "test_duration_seconds": 120,  # 2 minutes - enough for 30s k6 run + cooldown
            "warmup_seconds": 0,
            "cooldown_seconds": 0,
            "output_dir": str(output_dir),
            "report_dir": str(report_dir),
            "data_export_dir": str(export_dir),
            "remote_hosts": [
                {
                    "name": target_vm_name,
                    "address": target_vm["ip"],
                    "user": target_vm["user"],
                    "become": True,
                    "vars": {
                        "ansible_ssh_private_key_file": str(staged_key),
                        "ansible_ssh_common_args": "-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null",
                    },
                }
            ],
            "remote_execution": {
                "enabled": True,
                "run_setup": False,
                "run_collect": True,
                "run_teardown": False,  # Keep lb_workdir for debugging
                "lb_workdir": lb_workdir,
            },
            "plugin_settings": {
                "dfaas": dfaas_options,
            },
            "plugin_assets": {
                "dfaas": {"setup_playbook": None, "teardown_playbook": None}
            },
            "workloads": {
                "dfaas": {
                    "plugin": "dfaas",
                    "options": dfaas_options,
                }
            },
            "collectors": {
                "psutil_interval": 1.0,
                "cli_commands": [],
                "enable_ebpf": False,
            },
        }
    
        config_path = tmp_path / "benchmark_config.dfaas_multipass.json"
        config_path.write_text(json.dumps(config, indent=2))
    
        # Execute benchmark via CLI
        os.environ["OBJC_DISABLE_INITIALIZE_FORK_SAFETY"] = "YES"
        os.environ["LB_LOG_LEVEL"] = "DEBUG"
    
        print(f"Executing: uv run lb run --remote -c {config_path}")
        result = subprocess.run(
            ["uv", "run", "lb", "run", "--remote", "-c", str(config_path)],
            capture_output=True,
            text=True,
            timeout=600,
            cwd=Path(__file__).parent.parent.parent,  # Project root
            env=os.environ,
        )
    
        print(f"CLI exit code: {result.returncode}")
        if result.stdout:
            print(f"CLI stdout:\n{result.stdout[:20000]}")
        if result.stderr:
            print(f"CLI stderr:\n{result.stderr[:2000]}")
    
        cli_failed = result.returncode != 0
    
        # Capture diagnostics BEFORE failing so we can inspect the VMs
        if cli_failed:
            print("\n" + "=" * 60)
            print("CLI FAILED - CAPTURING DIAGNOSTICS BEFORE CLEANUP")
            print("=" * 60)
    
            try:
                opt_lb = _multipass_exec(
                    target_vm["name"],
                    ["bash", "-c", f"""
                    echo "=== {lb_workdir} directory ==="
                    ls -la {lb_workdir}/ 2>/dev/null || echo "{lb_workdir} not found"
    
                    echo ""
                    echo "=== LocalRunner status file ==="
                    cat {lb_workdir}/lb_localrunner.status.json 2>/dev/null || echo "No status file"
    
                    echo ""
                    echo "=== LocalRunner PID file ==="
                    cat {lb_workdir}/lb_localrunner.pid 2>/dev/null || echo "No PID file"
    
                    echo ""
                    echo "=== benchmark_config.generated.json (first 200 lines) ==="
                    head -200 {lb_workdir}/benchmark_config.generated.json 2>/dev/null || echo "No generated config"
    
                    echo ""
                    echo "=== Event stream log content ==="
                    cat /tmp/benchmark_results/*/benchmark-test-vm-*/lb_events.stream.log 2>/dev/null || echo "No stream log"
    
                    echo ""
                    echo "=== All files in {lb_workdir} ==="
                    find {lb_workdir} -type f 2>/dev/null | head -30
    
                    echo ""
                    echo "=== Remote benchmark_results content ==="
                    find /tmp -path '*benchmark_results*' -type f 2>/dev/null | head -30
                    """],
                )
                print(opt_lb.stdout)
            except Exception as e:
                print(f"Failed to capture diagnostics: {e}")
    
            _skip_or_fail(
                f"CLI execution failed with exit code {result.returncode}",
                context="cli_execution",
            )
    
        # Verify LB_EVENT lines in output
        lb_event_lines = [line for line in result.stdout.split("\n") if "LB_EVENT" in line]
        print(f"Found {len(lb_event_lines)} LB_EVENT lines in CLI output")
    
        # Verify artifact structure on local output directory
        dfaas_output = output_dir / "dfaas"
        if not dfaas_output.exists():
            # Try nested structure
            run_dirs = list(output_dir.glob("run-*"))
            if run_dirs:
                dfaas_output = run_dirs[0] / "dfaas"
            else:
                # List what we have locally
                print(f"Contents of local output_dir: {list(output_dir.iterdir()) if output_dir.exists() else 'does not exist'}")
    
                # Check remote output directory
                print("Checking remote output directory content...")
                try:
                    remote_ls = _multipass_exec(target_vm["name"], ["ls", "-laR", "/tmp/lb_e2e_results"])
                    print(f"Remote /tmp/lb_e2e_results content:\n{remote_ls.stdout}")
                except Exception as e:
                    print(f"Failed to list remote directory: {e}")
    
                pytest.fail(f"DFaaS output directory not found in {output_dir}")
    
        # Debug: Print journal and run log (critical for diagnosing setup failures)
        for path in output_dir.rglob("run_journal.json"):
            print(f"--- Journal: {path} ---")
            try:
                content = path.read_text()
                print(content[:5000])
            except Exception as e:
                print(f"Failed to read journal: {e}")
    
        for path in output_dir.rglob("run.log"):
            print(f"\n--- Run Log: {path} ---")
            try:
                content = path.read_text()
                # Print last 10000 chars to see setup.yml output
                if len(content) > 10000:
                    print(f"... (truncated, showing last 10000 chars of {len(content)} total) ...")
                    print(content[-10000:])
                else:
                    print(content)
            except Exception as e:
                print(f"Failed to read run log: {e}")
    
        k6_config_ids: list[str] = []
        if dfaas_output.exists():
            _verify_dfaas_artifact_structure(dfaas_output)
            _verify_results_csv_content(dfaas_output / "results.csv")
            k6_config_ids = _extract_config_ids_from_summary_files(
                dfaas_output / "summaries"
            )
        else:
            logger.warning("DFaaS output directory not found locally - checking remote")
    
        # Debug: Check LocalRunner output and DFaaS generator output on target VM
        print("\n" + "=" * 60)
        print("DIAGNOSING DFAAS EXECUTION ON TARGET VM")
        print("=" * 60)
    
        # With run_teardown=False, lb_workdir should still exist
        print(f"\n--- {lb_workdir} Contents ---")
        try:
            opt_lb = _multipass_exec(
                target_vm["name"],
                ["bash", "-c", f"""
                echo "=== {lb_workdir} directory ==="
                ls -la {lb_workdir}/ 2>/dev/null || echo "{lb_workdir} not found"
    
                echo ""
                echo "=== LocalRunner status file ==="
                cat {lb_workdir}/lb_localrunner.status.json 2>/dev/null || echo "No status file"
    
                echo ""
                echo "=== LocalRunner PID file ==="
                cat {lb_workdir}/lb_localrunner.pid 2>/dev/null || echo "No PID file"
    
                echo ""
                echo "=== benchmark_config.generated.json (first 200 lines) ==="
                head -200 {lb_workdir}/benchmark_config.generated.json 2>/dev/null || echo "No generated config"
    
                echo ""
                echo "=== Event stream log ==="
                cat /tmp/benchmark_results/*/benchmark-test-vm-*/lb_events.stream.log 2>/dev/null || echo "No stream log"
                """],
            )
            print(opt_lb.stdout)
        except Exception as e:
            print(f"Failed to check {lb_workdir}: {e}")
    
        # Check for LocalRunner execution evidence in /tmp
        print("\n--- LocalRunner Execution Evidence ---")
        try:
            runner_evidence = _multipass_exec(
                target_vm["name"],
                ["bash", "-c", """
                echo "=== Runner logs in /tmp ==="
                ls -la /tmp/lb_localrunner*.log 2>/dev/null || echo "No runner logs found"
    
                echo ""
                echo "=== Benchmark results ==="
                find /tmp -path '*benchmark_results*' -type f 2>/dev/null | head -20 || echo "No results found"
    
                echo ""
                echo "=== DFaaS-related files ==="
                find /tmp /home/ubuntu -name '*dfaas*' -type f 2>/dev/null | head -10 || echo "No dfaas files"
    
                echo ""
                echo "=== Process history (if available) ==="
                grep -l -E "lb|dfaas|k6" /var/log/syslog 2>/dev/null | head -5 || echo "No relevant syslog entries"
                """],
            )
            print(runner_evidence.stdout)
        except Exception as e:
            print(f"Failed to check runner evidence: {e}")
    
        # Check for any dfaas-related output
        try:
            dfaas_find = _multipass_exec(
                target_vm["name"],
                ["bash", "-c", f"find /tmp {lb_workdir} /home/ubuntu -name '*dfaas*' -o -name '*results.csv*' 2>/dev/null | head -20"]
            )
            print(f"DFaaS-related files on target:\n{dfaas_find.stdout}")
        except Exception as e:
            print(f"Failed to find dfaas files: {e}")
    
        # Check LocalRunner stderr/stdout logs
        try:
            runner_logs = _multipass_exec(
                target_vm["name"],
                ["bash", "-c", "cat /tmp/lb_localrunner*.log 2>/dev/null || echo 'No LocalRunner logs found'"]
            )
            if "No LocalRunner logs" not in runner_logs.stdout:
                print(f"LocalRunner logs:\n{runner_logs.stdout[-3000:]}")
        except Exception as e:
            print(f"Failed to read LocalRunner logs: {e}")
    
        # Check benchmark results
        try:
            results_find = _multipass_exec(
                target_vm["name"],
                ["bash", "-c", "find /tmp -path '*benchmark_results*' -name '*.csv' 2>/dev/null"]
            )
            print(f"CSV files in benchmark_results:\n{results_find.stdout}")
            for csv_path in results_find.stdout.strip().split('\n'):
                if csv_path and 'results.csv' in csv_path:
                    try:
                        csv_content = _remote_read_file(target_vm["name"], csv_path)
                        print(f"--- Remote {csv_path} ---")
                        print(csv_content[:2000])
                    except Exception:
                        pass
        except Exception as e:
            print(f"Failed to find remote CSVs: {e}")
    
        print("=" * 60 + "\n")
    
        # Verify k6 logs on generator VM (now with better context)
        # This is a critical check - if k6 didn't run, the DFaaS benchmark didn't execute
        try:
>           _verify_k6_logs_on_generator(
                k6_vm["name"],
                k6_workspace_root,
                target_vm_name=target_vm["name"],
                config_ids=k6_config_ids,
            )

tests/e2e/test_dfaas_multipass_e2e.py:2504: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

vm_name = 'dfaas-generator', workspace_root = '/home/ubuntu/.dfaas-k6'
target_vm_name = 'dfaas-target', cli_stdout = None, config_ids = []

    def _verify_k6_logs_on_generator(
        vm_name: str,
        workspace_root: str,
        target_vm_name: str | None = None,
        cli_stdout: str | None = None,
        config_ids: list[str] | None = None,
    ) -> None:
        """Verify k6 generated logs on the generator VM."""
        print(f"Verifying k6 logs on {vm_name} in {workspace_root}")
    
        # Check if we saw streamed logs in CLI output
        if cli_stdout and "k6[" in cli_stdout and "stdout:" in cli_stdout:
            print("âœ“ Confirmed k6 logs were streamed to CLI output")
            return
    
        if config_ids:
            _verify_k6_workspace_artifacts(vm_name, workspace_root, config_ids)
            return
    
        k6_logs = _remote_find_files(vm_name, workspace_root, "k6.log")
        print(f"Found k6 logs on generator: {k6_logs}")
    
        if not k6_logs:
            print(f"No k6 logs found on generator {vm_name}. Listing {workspace_root} recursively with sudo:")
            try:
                ls_cmd = f"sudo ls -laR {workspace_root}; echo 'Hostname:'; hostname"
                ls_result = _multipass_exec(vm_name, ["bash", "-c", ls_cmd])
                print(ls_result.stdout)
            except Exception as e:
                print(f"Failed to list directory on generator: {e}")
    
            if target_vm_name:
                print(f"Checking target VM {target_vm_name} for misplaced logs...")
                target_logs = _remote_find_files(target_vm_name, workspace_root, "k6.log")
                print(f"Found k6 logs on target: {target_logs}")
                if target_logs:
                    print("WARNING: k6 logs found on TARGET VM, not GENERATOR VM!")
                    return
    
>       assert len(k6_logs) > 0, f"k6.log files found on generator in {workspace_root} or logs streamed in CLI output"
E       AssertionError: k6.log files found on generator in /home/ubuntu/.dfaas-k6 or logs streamed in CLI output
E       assert 0 > 0
E        +  where 0 = len([])

tests/e2e/test_dfaas_multipass_e2e.py:1245: AssertionError

During handling of the above exception, another exception occurred:

multipass_two_vms = [{'ip': '192.168.2.2', 'key_path': '/Users/micheleciavotta/Downloads/linux-benchmark-lib/temp_keys/dfaas_multipass_key...eleciavotta/Downloads/linux-benchmark-lib/temp_keys/dfaas_multipass_key', 'name': 'dfaas-generator', 'user': 'ubuntu'}]
tmp_path = PosixPath('/private/var/folders/q1/5ln0gtyj3fv55055frjp0nx80000gn/T/pytest-of-micheleciavotta/pytest-374/test_dfaas_multipass_cli_workf0')

    def test_dfaas_multipass_cli_workflow(multipass_two_vms, tmp_path: Path) -> None:
        """E2E test that executes DFaaS benchmark via CLI (lb run --remote).
    
        This test verifies the complete DFaaS workflow as described in the prompt:
        1. Infrastructure Setup: Two VMs (target + generator) with SSH access
        2. Configuration Generation: Dynamic JSON config file
        3. Benchmark Execution: Via `uv run lb run --remote -c <config>`
        4. Verification: Artifacts, results content, k6 logs, and LB_EVENT output
    
        Uses minimal test config: rate=10, duration=10s, single function.
        """
        _ensure_local_prereqs()
        target_vm, k6_vm = multipass_two_vms[0], multipass_two_vms[1]
        k6_workspace_root = _k6_workspace_root(k6_vm["user"])
    
        ansible_dir = tmp_path / "ansible_cli"
        staged_key = stage_private_key(Path(target_vm["key_path"]),
                                       ansible_dir / "keys")
        target_host = {
            "ip": target_vm["ip"],
            "user": target_vm["user"],
            "key": str(staged_key),
        }
        k6_host = {
            "ip": k6_vm["ip"],
            "user": k6_vm["user"],
            "key": str(staged_key),
        }
    
        ansible_env = make_test_ansible_env(ansible_dir)
        target_inventory = ansible_dir / "target_inventory.ini"
        k6_inventory = ansible_dir / "k6_inventory.ini"
        _write_inventory(target_inventory, target_host)
        _write_inventory(k6_inventory, k6_host)
    
        # Setup target (k6 provisioning is executed from target)
        setup_target = Path("lb_plugins/plugins/dfaas/ansible/setup_target.yml")
        setup_k6 = Path("lb_plugins/plugins/dfaas/ansible/setup_k6.yml")
    
        # Use fixed /tmp path for output to avoid cross-platform path issues (macOS -> Linux VM)
        output_dir = Path("/tmp/lb_e2e_results")
        if output_dir.exists():
            shutil.rmtree(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        lb_workdir = _lb_workdir(target_vm["user"])
    
        report_dir = tmp_path / "reports"
        export_dir = tmp_path / "exports"
    
        k6_key_target_path = f"/home/{target_vm['user']}/.ssh/dfaas_k6_key"
        k6_playbook_target_path = "/tmp/setup_k6.yml"
        try:
            _copy_private_key_to_target(
                target_vm["name"], staged_key, k6_key_target_path
            )
            _copy_file_to_target(
                target_vm["name"],
                setup_k6,
                k6_playbook_target_path,
            )
        except Exception as exc:  # noqa: BLE001
            _skip_or_fail("Failed to copy k6 SSH key to target", context="copy_k6_key_cli", exc=exc)
    
        try:
            _run_playbook(
                setup_target,
                target_inventory,
                {
                    "openfaas_functions": ["env"],
                    "k6_host": k6_vm["ip"],
                    "k6_user": k6_vm["user"],
                    "k6_ssh_key": k6_key_target_path,
                    "k6_port": 22,
                    "k6_workspace_root": k6_workspace_root,
                    "k6_setup_playbook_path": k6_playbook_target_path,
                },
                ansible_env,
            )
        except Exception as exc:  # noqa: BLE001
            _skip_or_fail("setup_target playbook failed", context="setup_target_cli", exc=exc)
    
        try:
            password = _get_openfaas_password(target_vm["name"])
        except Exception as exc:  # noqa: BLE001
            _skip_or_fail("Failed to read OpenFaaS password", context="openfaas_password_cli", exc=exc)
    
        auth_value = base64.b64encode(f"admin:{password}".encode("utf-8")).decode("utf-8")
    
        # DFaaS plugin options - used in both plugin_settings and workloads.options
        dfaas_options = {
            "gateway_url": f"http://{target_vm['ip']}:31112",
            "prometheus_url": "http://127.0.0.1:30411",
            "k6_host": k6_vm["ip"],
            "k6_user": k6_vm["user"],
            "k6_ssh_key": k6_key_target_path,
            "k6_port": 22,
            "k6_workspace_root": k6_workspace_root,
            "k6_log_stream": True,
            "functions": [
                {
                    "name": "env",
                    "method": "GET",
                    "body": "",
                    "headers": {"Authorization": f"Basic {auth_value}"},
                }
            ],
            "rates": {"min_rate": 10, "max_rate": 10, "step": 10},
            "combinations": {"min_functions": 1, "max_functions": 2},
            "duration": "30s",
            "iterations": 1,
            "cooldown": {
                "max_wait_seconds": 60,
                "sleep_step_seconds": 5,
                "idle_threshold_pct": 20,
            },
        }
    
        target_vm_name = target_vm["name"]
    
        config = {
            "repetitions": 1,
            "test_duration_seconds": 120,  # 2 minutes - enough for 30s k6 run + cooldown
            "warmup_seconds": 0,
            "cooldown_seconds": 0,
            "output_dir": str(output_dir),
            "report_dir": str(report_dir),
            "data_export_dir": str(export_dir),
            "remote_hosts": [
                {
                    "name": target_vm_name,
                    "address": target_vm["ip"],
                    "user": target_vm["user"],
                    "become": True,
                    "vars": {
                        "ansible_ssh_private_key_file": str(staged_key),
                        "ansible_ssh_common_args": "-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null",
                    },
                }
            ],
            "remote_execution": {
                "enabled": True,
                "run_setup": False,
                "run_collect": True,
                "run_teardown": False,  # Keep lb_workdir for debugging
                "lb_workdir": lb_workdir,
            },
            "plugin_settings": {
                "dfaas": dfaas_options,
            },
            "plugin_assets": {
                "dfaas": {"setup_playbook": None, "teardown_playbook": None}
            },
            "workloads": {
                "dfaas": {
                    "plugin": "dfaas",
                    "options": dfaas_options,
                }
            },
            "collectors": {
                "psutil_interval": 1.0,
                "cli_commands": [],
                "enable_ebpf": False,
            },
        }
    
        config_path = tmp_path / "benchmark_config.dfaas_multipass.json"
        config_path.write_text(json.dumps(config, indent=2))
    
        # Execute benchmark via CLI
        os.environ["OBJC_DISABLE_INITIALIZE_FORK_SAFETY"] = "YES"
        os.environ["LB_LOG_LEVEL"] = "DEBUG"
    
        print(f"Executing: uv run lb run --remote -c {config_path}")
        result = subprocess.run(
            ["uv", "run", "lb", "run", "--remote", "-c", str(config_path)],
            capture_output=True,
            text=True,
            timeout=600,
            cwd=Path(__file__).parent.parent.parent,  # Project root
            env=os.environ,
        )
    
        print(f"CLI exit code: {result.returncode}")
        if result.stdout:
            print(f"CLI stdout:\n{result.stdout[:20000]}")
        if result.stderr:
            print(f"CLI stderr:\n{result.stderr[:2000]}")
    
        cli_failed = result.returncode != 0
    
        # Capture diagnostics BEFORE failing so we can inspect the VMs
        if cli_failed:
            print("\n" + "=" * 60)
            print("CLI FAILED - CAPTURING DIAGNOSTICS BEFORE CLEANUP")
            print("=" * 60)
    
            try:
                opt_lb = _multipass_exec(
                    target_vm["name"],
                    ["bash", "-c", f"""
                    echo "=== {lb_workdir} directory ==="
                    ls -la {lb_workdir}/ 2>/dev/null || echo "{lb_workdir} not found"
    
                    echo ""
                    echo "=== LocalRunner status file ==="
                    cat {lb_workdir}/lb_localrunner.status.json 2>/dev/null || echo "No status file"
    
                    echo ""
                    echo "=== LocalRunner PID file ==="
                    cat {lb_workdir}/lb_localrunner.pid 2>/dev/null || echo "No PID file"
    
                    echo ""
                    echo "=== benchmark_config.generated.json (first 200 lines) ==="
                    head -200 {lb_workdir}/benchmark_config.generated.json 2>/dev/null || echo "No generated config"
    
                    echo ""
                    echo "=== Event stream log content ==="
                    cat /tmp/benchmark_results/*/benchmark-test-vm-*/lb_events.stream.log 2>/dev/null || echo "No stream log"
    
                    echo ""
                    echo "=== All files in {lb_workdir} ==="
                    find {lb_workdir} -type f 2>/dev/null | head -30
    
                    echo ""
                    echo "=== Remote benchmark_results content ==="
                    find /tmp -path '*benchmark_results*' -type f 2>/dev/null | head -30
                    """],
                )
                print(opt_lb.stdout)
            except Exception as e:
                print(f"Failed to capture diagnostics: {e}")
    
            _skip_or_fail(
                f"CLI execution failed with exit code {result.returncode}",
                context="cli_execution",
            )
    
        # Verify LB_EVENT lines in output
        lb_event_lines = [line for line in result.stdout.split("\n") if "LB_EVENT" in line]
        print(f"Found {len(lb_event_lines)} LB_EVENT lines in CLI output")
    
        # Verify artifact structure on local output directory
        dfaas_output = output_dir / "dfaas"
        if not dfaas_output.exists():
            # Try nested structure
            run_dirs = list(output_dir.glob("run-*"))
            if run_dirs:
                dfaas_output = run_dirs[0] / "dfaas"
            else:
                # List what we have locally
                print(f"Contents of local output_dir: {list(output_dir.iterdir()) if output_dir.exists() else 'does not exist'}")
    
                # Check remote output directory
                print("Checking remote output directory content...")
                try:
                    remote_ls = _multipass_exec(target_vm["name"], ["ls", "-laR", "/tmp/lb_e2e_results"])
                    print(f"Remote /tmp/lb_e2e_results content:\n{remote_ls.stdout}")
                except Exception as e:
                    print(f"Failed to list remote directory: {e}")
    
                pytest.fail(f"DFaaS output directory not found in {output_dir}")
    
        # Debug: Print journal and run log (critical for diagnosing setup failures)
        for path in output_dir.rglob("run_journal.json"):
            print(f"--- Journal: {path} ---")
            try:
                content = path.read_text()
                print(content[:5000])
            except Exception as e:
                print(f"Failed to read journal: {e}")
    
        for path in output_dir.rglob("run.log"):
            print(f"\n--- Run Log: {path} ---")
            try:
                content = path.read_text()
                # Print last 10000 chars to see setup.yml output
                if len(content) > 10000:
                    print(f"... (truncated, showing last 10000 chars of {len(content)} total) ...")
                    print(content[-10000:])
                else:
                    print(content)
            except Exception as e:
                print(f"Failed to read run log: {e}")
    
        k6_config_ids: list[str] = []
        if dfaas_output.exists():
            _verify_dfaas_artifact_structure(dfaas_output)
            _verify_results_csv_content(dfaas_output / "results.csv")
            k6_config_ids = _extract_config_ids_from_summary_files(
                dfaas_output / "summaries"
            )
        else:
            logger.warning("DFaaS output directory not found locally - checking remote")
    
        # Debug: Check LocalRunner output and DFaaS generator output on target VM
        print("\n" + "=" * 60)
        print("DIAGNOSING DFAAS EXECUTION ON TARGET VM")
        print("=" * 60)
    
        # With run_teardown=False, lb_workdir should still exist
        print(f"\n--- {lb_workdir} Contents ---")
        try:
            opt_lb = _multipass_exec(
                target_vm["name"],
                ["bash", "-c", f"""
                echo "=== {lb_workdir} directory ==="
                ls -la {lb_workdir}/ 2>/dev/null || echo "{lb_workdir} not found"
    
                echo ""
                echo "=== LocalRunner status file ==="
                cat {lb_workdir}/lb_localrunner.status.json 2>/dev/null || echo "No status file"
    
                echo ""
                echo "=== LocalRunner PID file ==="
                cat {lb_workdir}/lb_localrunner.pid 2>/dev/null || echo "No PID file"
    
                echo ""
                echo "=== benchmark_config.generated.json (first 200 lines) ==="
                head -200 {lb_workdir}/benchmark_config.generated.json 2>/dev/null || echo "No generated config"
    
                echo ""
                echo "=== Event stream log ==="
                cat /tmp/benchmark_results/*/benchmark-test-vm-*/lb_events.stream.log 2>/dev/null || echo "No stream log"
                """],
            )
            print(opt_lb.stdout)
        except Exception as e:
            print(f"Failed to check {lb_workdir}: {e}")
    
        # Check for LocalRunner execution evidence in /tmp
        print("\n--- LocalRunner Execution Evidence ---")
        try:
            runner_evidence = _multipass_exec(
                target_vm["name"],
                ["bash", "-c", """
                echo "=== Runner logs in /tmp ==="
                ls -la /tmp/lb_localrunner*.log 2>/dev/null || echo "No runner logs found"
    
                echo ""
                echo "=== Benchmark results ==="
                find /tmp -path '*benchmark_results*' -type f 2>/dev/null | head -20 || echo "No results found"
    
                echo ""
                echo "=== DFaaS-related files ==="
                find /tmp /home/ubuntu -name '*dfaas*' -type f 2>/dev/null | head -10 || echo "No dfaas files"
    
                echo ""
                echo "=== Process history (if available) ==="
                grep -l -E "lb|dfaas|k6" /var/log/syslog 2>/dev/null | head -5 || echo "No relevant syslog entries"
                """],
            )
            print(runner_evidence.stdout)
        except Exception as e:
            print(f"Failed to check runner evidence: {e}")
    
        # Check for any dfaas-related output
        try:
            dfaas_find = _multipass_exec(
                target_vm["name"],
                ["bash", "-c", f"find /tmp {lb_workdir} /home/ubuntu -name '*dfaas*' -o -name '*results.csv*' 2>/dev/null | head -20"]
            )
            print(f"DFaaS-related files on target:\n{dfaas_find.stdout}")
        except Exception as e:
            print(f"Failed to find dfaas files: {e}")
    
        # Check LocalRunner stderr/stdout logs
        try:
            runner_logs = _multipass_exec(
                target_vm["name"],
                ["bash", "-c", "cat /tmp/lb_localrunner*.log 2>/dev/null || echo 'No LocalRunner logs found'"]
            )
            if "No LocalRunner logs" not in runner_logs.stdout:
                print(f"LocalRunner logs:\n{runner_logs.stdout[-3000:]}")
        except Exception as e:
            print(f"Failed to read LocalRunner logs: {e}")
    
        # Check benchmark results
        try:
            results_find = _multipass_exec(
                target_vm["name"],
                ["bash", "-c", "find /tmp -path '*benchmark_results*' -name '*.csv' 2>/dev/null"]
            )
            print(f"CSV files in benchmark_results:\n{results_find.stdout}")
            for csv_path in results_find.stdout.strip().split('\n'):
                if csv_path and 'results.csv' in csv_path:
                    try:
                        csv_content = _remote_read_file(target_vm["name"], csv_path)
                        print(f"--- Remote {csv_path} ---")
                        print(csv_content[:2000])
                    except Exception:
                        pass
        except Exception as e:
            print(f"Failed to find remote CSVs: {e}")
    
        print("=" * 60 + "\n")
    
        # Verify k6 logs on generator VM (now with better context)
        # This is a critical check - if k6 didn't run, the DFaaS benchmark didn't execute
        try:
            _verify_k6_logs_on_generator(
                k6_vm["name"],
                k6_workspace_root,
                target_vm_name=target_vm["name"],
                config_ids=k6_config_ids,
            )
        except AssertionError as e:
            # Provide actionable failure message
>           pytest.fail(
                f"k6 logs not found on generator VM. This means the DFaaS benchmark "
                f"did not execute k6. Possible causes:\n"
                f"1. LocalRunner didn't start on target (check run.log above)\n"
                f"2. DFaaS generator failed to connect to k6 host\n"
                f"3. SSH from target to generator failed during execution\n"
                f"Original error: {e}"
            )
E           Failed: k6 logs not found on generator VM. This means the DFaaS benchmark did not execute k6. Possible causes:
E           1. LocalRunner didn't start on target (check run.log above)
E           2. DFaaS generator failed to connect to k6 host
E           3. SSH from target to generator failed during execution
E           Original error: k6.log files found on generator in /home/ubuntu/.dfaas-k6 or logs streamed in CLI output
E           assert 0 > 0
E            +  where 0 = len([])

tests/e2e/test_dfaas_multipass_e2e.py:2512: Failed
----------------------------- Captured stdout call -----------------------------
Executing: uv run lb run --remote -c /private/var/folders/q1/5ln0gtyj3fv55055frjp0nx80000gn/T/pytest-of-micheleciavotta/pytest-374/test_dfaas_multipass_cli_workf0/benchmark_config.dfaas_multipass.json
CLI exit code: 0
CLI stdout:
âœ” Loaded config: 
/private/var/folders/q1/5ln0gtyj3fv55055frjp0nx80000gn/T/pytest-of-micheleciavot
ta/pytest-374/test_dfaas_multipass_cli_workf0/benchmark_config.dfaas_multipass.j
son
                                   Run Plan                                   
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Workload  â”‚ Plugin  â”‚ Intensity     â”‚ Configuratiâ€¦ â”‚ Repetitions  â”‚ Status â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dfaas     â”‚ dfaas   â”‚ user_defined  â”‚ max_retriesâ€¦ â”‚ 1            â”‚ Remote â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â„¹ Run run-20260109-205250 completed in 12.4s
â„¹ Controller state: ControllerState.FAILED
âš  Leaving provisioned nodes for inspection
                    Run Journal (ID: run-20260109-205250)                     
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Host                  â”‚ Workload       â”‚ Run          â”‚ Last Action        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dfaas-target          â”‚ dfaas          â”‚ pending      â”‚                    â”‚
â”‚                       â”‚                â”‚ 0/1          â”‚                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â„¹ Journal saved to /tmp/lb_e2e_results/run-20260109-205250/run_journal.json
â„¹ Ansible output log saved to /tmp/lb_e2e_results/run-20260109-205250/run.log
â„¹ Dashboard log stream saved to 
/tmp/lb_e2e_results/run-20260109-205250/ui_stream.log
âœ” Run completed.

CLI stderr:
[2m2026-01-09T20:52:50.308030Z[0m [[32m[1mdebug    [0m] [1mPTS plugin bundle created 5 workload(s): ["pts_build_linux_kernel", "pts_compress_7zip", "pts_gmpbench", "pts_ramspeed", "pts_blosc"][0m
[2m2026-01-09T20:52:50.392212Z[0m [[32m[1mdebug    [0m] [1mSkipping /Users/micheleciavotta/Downloads/linux-benchmark-lib/lb_plugins/plugins/_user/dummy_plugin/__init__.py: no PLUGIN/PLUGINS/get_plugins exports found[0m
[2m2026-01-09T20:52:50.392679Z[0m [[32m[1mdebug    [0m] [1mSkipping /Users/micheleciavotta/Downloads/linux-benchmark-lib/lb_plugins/plugins/_user/remote/__init__.py: no PLUGIN/PLUGINS/get_plugins exports found[0m
[2m2026-01-09T20:52:50.622292Z[0m [[32m[1minfo     [0m] [1mController state: running_global_setup[0m
[2m2026-01-09T20:52:50.624884Z[0m [[32m[1minfo     [0m] [1mStarting Run run-20260109-205250[0m
[2m2026-01-09T20:52:50.624998Z[0m [[32m[1minfo     [0m] [1mPhase: Global Setup           [0m
[2m2026-01-09T20:52:50.625101Z[0m [[32m[1minfo     [0m] [1mExecuting global setup playbook[0m
[2m2026-01-09T20:52:50.644907Z[0m [[32m[1minfo     [0m] [1mRunning playbook setup.yml against 1 host(s)[0m
[2m2026-01-09T20:52:51.444440Z[0m [[32m[1mdebug    [0m] [1mSTREAM b'IHDR' 16 13          [0m
[2m2026-01-09T20:52:51.444757Z[0m [[32m[1mdebug    [0m] [1mSTREAM b'IDAT' 41 9896        [0m
[2m2026-01-09T20:53:01.134371Z[0m [[32m[1minfo     [0m] [1mPlaybook /Users/micheleciavotta/Downloads/linux-benchmark-lib/lb_controller/ansible/playbooks/setup.yml finished with rc=0 status=successful[0m
[2m2026-01-09T20:53:01.134591Z[0m [[32m[1minfo     [0m] [1mController state: running_workloads[0m
[2m2026-01-09T20:53:01.135812Z[0m [[32m[1minfo     [0m] [1mSetup: dfaas (dfaas)          [0m
[2m2026-01-09T20:53:01.135922Z[0m [[32m[1minfo     [0m] [1mExecuting setup playbook for dfaas (dfaas)[0m
[2m2026-01-09T20:53:01.136447Z[0m [[32m[1minfo     [0m] [1mRunning playbook setup_global.yml 
Found 0 LB_EVENT lines in CLI output
--- Journal: /tmp/lb_e2e_results/run-20260109-205250/run_journal.json ---
{
  "run_id": "run-20260109-205250",
  "tasks": [
    {
      "host": "dfaas-target",
      "workload": "dfaas",
      "repetition": 1,
      "status": "PENDING",
      "current_action": "",
      "timestamp": 1767991970.619077,
      "error": null,
      "started_at": null,
      "finished_at": null,
      "duration_seconds": null
    }
  ],
  "metadata": {
    "created_at": "2026-01-09T21:52:50.618665",
    "config_summary": "repetitions=1 test_duration_seconds=120 metrics_interval_seconds=1.0 warmup_seconds=0 cooldown_seconds=0 output_dir=PosixPath('/tmp/lb_e2e_results') report_dir=PosixPath('/private/var/folders/q1/5ln0gtyj3fv55055frjp0nx80000gn/T/pytest-of-micheleciavotta/pytest-374/test_dfaas_multipass_cli_workf0/reports') data_export_dir=PosixPath('/private/var/folders/q1/5ln0gtyj3fv55055frjp0nx80000gn/T/pytest-of-micheleciavotta/pytest-374/test_dfaas_multipass_cli_workf0/exports') plugin_settings={'dfaas': DfaasConfig(max_retries=0, timeout_buffer=10, tags=[], config_path=None, output_dir=None, run_id=None, k6_host='192.168.2.3', k6_user='ubuntu', k6_ssh_key='/home/ubuntu/.ssh/dfaas_k6_key', k6_port=22, k6_workspace_root='/home/ubuntu/.dfaas-k6', k6_log_stream=True, k6_outputs=[], k6_tags={}, openfaas_port=31112, prometheus_port=30411, gateway_url='http://192.168.2.2:31112', prometheus_url='http://127.0.0.1:30411', functions=[DfaasFunctionConfig(name='env', method='GET', body='', headers={'Authorization': 'Basic YWRtaW46WUEwNFhPaDVlUWNk'}, max_rate=None)], rates=DfaasRatesConfig(min_rate=10, max_rate=10, step=10), combinations=DfaasCombinationConfig(min_functions=1, max_functions=2), duration='30s', iterations=1, cooldown=DfaasCooldownConfig(max_wait_seconds=60, sleep_step_seconds=5, idle_threshold_pct=20.0), overload=DfaasOverloadConfig(cpu_overload_pct_of_capacity=80, ram_overload_pct=90, success_rate_node_min=0.95, success_rate_function_min=0.9, replicas_overload_threshold=15), queries_path='/Users/micheleciavotta/Downloads/linux-benchmark-lib/lb_plugins/plugins/dfaas/queries.yml', deploy_functions=True, scaphandre_enabled=False, function_pid_regexes={}, grafana=GrafanaConfig(enabled=False, url='http://localhost:3000', api_key=None, org_id=1), loki=DfaasLokiConfig(enabled=False, endpoint='http://localhost:3100', labels={}))} plugin_assets={'stress_ng': PluginAssetConfig(setup_playbook=PosixPath('/Users/micheleciavotta/Downloads/linux-benchmark-lib/lb_plugins/plugins/stress_ng/ansible/setup.yml'), teardown_playbook=PosixPath('/Users/micheleciavotta/Downloads/linux-benchmark-lib/lb_plugins/plugins/stress_ng/ansible/teardown.yml'), setup_extravars={}, teardown_extravars={}), 'fio': PluginAssetConfig(setup_playbook=PosixPath('/Users/micheleciavotta/Downloads/linux-benchmark-lib/lb_plugins/plugins/fio/ansible/setup.yml'), teardown_playbook=None, setup_extravars={}, teardown_extravars={}), 'unixbench': PluginAssetConfig(setup_playbook=PosixPath('/Users/micheleciavotta/Downloads/linux-benchmark-lib/lb_plugins/plugins/unixbench/ansible/setup.yml'), teardown_playbook=None, setup_extravars={}, teardown_extravars={}), 'stream': PluginAssetConfig(setup_playbook=PosixPath('/Users/micheleciavotta/Downloads/linux-benchmark-lib/lb_plugins/plugins/stream/ansible/setup.yml'), teardown_playbook=None, setup_extravars={}, teardown_extravars={}), 'baseline': PluginAssetConfig(setup_playbook=None, teardown_playbook=None, setup_extravars={}, teardown_extravars={}), 'hpl': PluginAssetConfig(setup_playbook=PosixPath('/Users/micheleciavotta/Downloads/linux-benchmark-lib/lb_plugins/plugins/hpl/ansible/setup.yml'), teardown_playbook=None, setup_extravars={}, teardown_extravars={}), 'dd': PluginAssetConfig(setup_playbook=PosixPath('/Users/micheleciavotta/Downloads/linux-benchmark-lib/lb_plugins/plugins/dd/ansible/setup.yml'), teardown_playbook=None, setup_extravars={}, teardown_extravars={}), 'yabs': PluginAssetConfig(setup_playbook=PosixPath('/Users/micheleciavotta/Downloads/linux-benchmark-lib/lb_plugins/plugins/yabs/ansible/setup.yml'), teardown_playbook=None, setup_extravars={}, teardown_extravars={}), 'pts_build_linux_kernel': PluginAssetConfig(setup_playbook=PosixPath('/Users/micheleciavotta/Downloads/linux-benchmark-lib/lb_plugins/plugins/phoronix_test_suite/ansible/setup.yml'), teardown_playbook=PosixPath('/Users/micheleciavotta/Downloads/linux-benchmark-lib/lb_plugins/plugins/phoronix_test_suite/ansible/teardown.yml'), setup_extravars={'pts_profile': 'build-linux-kernel', 'pts_deb_relpath': 'lb_plugins/plugins/phoronix_test_suite/assets/phoronix-test-suite_10.8.4_all.deb', 'pts_home_root': '/Users/micheleciavotta/.lb/.phoronix-test-suite/', 'pts_apt_packages': ['apt-file', 'autoconf', 'bc', 'bison', 'build-essential', 'flex', 'gdebi-core', 'git', 'libssl-dev', 'mesa-utils', 'unzip', 'vulkan-tools']}, teardown_extravars={'pts_profile': 'build-linux-kernel', 'pts_deb_relpath': 'lb_plugins/plugins/phoronix_test_suite/assets/phoronix-test-suite_10.8.4_all.deb', 'pts_home_root': '/Users/micheleciavotta/.lb/.phoronix-test-

--- Run Log: /tmp/lb_e2e_results/run-20260109-205250/run.log ---
Controller state: running_global_setup

PLAY [Prepare benchmark hosts] *************************************************

TASK [Gathering Facts] *********************************************************
[WARNING]: Host 'dfaas-target' is using the discovered Python interpreter at '/usr/bin/python3.12', but future installation of another Python interpreter could cause a different interpreter to be discovered. See https://docs.ansible.com/ansible-core/2.20/reference_appendices/interpreter_discovery.html for more information.
ok: [dfaas-target]
LB_TASK {"host": "dfaas-target", "task": "Gathering Facts", "duration_s": 2.022, "status": "ok"}

TASK [Install collector dependencies (APT)] ************************************
ok: [dfaas-target]
LB_TASK {"host": "dfaas-target", "task": "Install collector dependencies (APT)", "duration_s": 0.832, "status": "ok"}

TASK [Create benchmark library workdir on remote host] *************************
ok: [dfaas-target]
LB_TASK {"host": "dfaas-target", "task": "Create benchmark library workdir on remote host", "duration_s": 0.319, "status": "ok"}

TASK [Ensure uv install directory exists] **************************************
ok: [dfaas-target]
LB_TASK {"host": "dfaas-target", "task": "Ensure uv install directory exists", "duration_s": 0.255, "status": "ok"}

TASK [Check for uv binary] *****************************************************
ok: [dfaas-target]
LB_TASK {"host": "dfaas-target", "task": "Check for uv binary", "duration_s": 0.462, "status": "ok"}

TASK [Download uv install script] **********************************************
skipping: [dfaas-target]
LB_TASK {"host": "dfaas-target", "task": "Download uv install script", "duration_s": 0.008, "status": "skipped"}

TASK [Install uv] **************************************************************
skipping: [dfaas-target]
LB_TASK {"host": "dfaas-target", "task": "Install uv", "duration_s": 0.008, "status": "skipped"}

TASK [Remove uv install script] ************************************************
skipping: [dfaas-target]
LB_TASK {"host": "dfaas-target", "task": "Remove uv install script", "duration_s": 0.008, "status": "skipped"}

TASK [Create local archive for benchmark library sources] **********************
changed: [dfaas-target -> localhost]
LB_TASK {"host": "dfaas-target", "task": "Create local archive for benchmark library sources", "duration_s": 0.323, "status": "ok"}

TASK [Store local archive path for all hosts] **********************************
ok: [dfaas-target -> localhost]
LB_TASK {"host": "dfaas-target", "task": "Store local archive path for all hosts", "duration_s": 0.011, "status": "ok"}

TASK [Build benchmark library archive locally] *********************************
changed: [dfaas-target -> localhost]
LB_TASK {"host": "dfaas-target", "task": "Build benchmark library archive locally", "duration_s": 0.981, "status": "ok"}

TASK [Upload benchmark library archive] ****************************************
changed: [dfaas-target]
LB_TASK {"host": "dfaas-target", "task": "Upload benchmark library archive", "duration_s": 0.724, "status": "ok"}

TASK [Extract benchmark library archive on remote host] ************************
changed: [dfaas-target]
LB_TASK {"host": "dfaas-target", "task": "Extract benchmark library archive on remote host", "duration_s": 1.073, "status": "ok"}

TASK [Remove remote benchmark library archive] *********************************
changed: [dfaas-target]
LB_TASK {"host": "dfaas-target", "task": "Remove remote benchmark library archive", "duration_s": 0.239, "status": "ok"}

TASK [Remove local benchmark library archive] **********************************
changed: [dfaas-target -> localhost]
LB_TASK {"host": "dfaas-target", "task": "Remove local benchmark library archive", "duration_s": 0.247, "status": "ok"}

TASK [Create uv virtual environment for benchmark library] *********************
ok: [dfaas-target]
LB_TASK {"host": "dfaas-target", "task": "Create uv virtual environment for benchmark library", "duration_s": 0.232, "status": "ok"}

TASK [Sync benchmark dependencies with uv (no dev)] ****************************
changed: [dfaas-target]
LB_TASK {"host": "dfaas-target", "task": "Sync benchmark dependencies with uv (no dev)", "duration_s": 1.222, "status": "ok"}

TASK [Ensure benchmark output directory exists] ********************************
changed: [dfaas-target]
LB_TASK {"host": "dfaas-target", "task": "Ensure benchmark output directory exists", "duration_s": 0.251, "status": "ok"}

TASK [Set ownership of benchmark library directory] ****************************
changed: [dfaas-target]
Controller state: running_workloads

PLAY [Register K6 Generator Host from Config] **********************************

TASK [Parse k6_host from config] ***********************************************
ok: [localhost]
LB_TASK {"host": "localhost", "task": "Parse k6_host from config", "duration_s": 0.018, "status": "ok"}

TASK [Add K6 host to inventory] ************************************************
changed: [localhost]
LB_TASK {"host": "localhost", "task": "Add K6 host to inventory", "duration_s": 0.02, "status": "ok"}

PLAY [Configure K6 Generator] **************************************************

TASK [Gathering Facts] *********************************************************
[ERROR]: Task failed: Failed to connect to the host via ssh: Warning: Permanently added '192.168.2.3' (ED25519) to the list of known hosts.
no such identity: /home/ubuntu/.ssh/dfaas_k6_key: No such file or directory
Controller state: running_global_teardown
Controller state: failed
Run run-20260109-205250 completed in 12.4s


============================================================
DIAGNOSING DFAAS EXECUTION ON TARGET VM
============================================================

--- /home/ubuntu/.lb Contents ---
=== /home/ubuntu/.lb directory ===
total 656
drwxr-xr-x 15 ubuntu ubuntu   4096 Jan  9 21:52 .
drwxr-x---  6 ubuntu ubuntu   4096 Jan  9 21:52 ..
-rw-r--r--  1 ubuntu ubuntu    163 Jan  3 23:34 ._README.md
-rwxr-xr-x  1 ubuntu ubuntu    163 Jan  3 23:34 ._lb_analytics
-rwxr-xr-x  1 ubuntu ubuntu    163 Jan  3 23:34 ._lb_app
-rwxr-xr-x  1 ubuntu ubuntu    163 Jan  4 21:51 ._lb_common
-rwxr-xr-x  1 ubuntu ubuntu    163 Jan  3 23:34 ._lb_controller
-rwxr-xr-x  1 ubuntu ubuntu    163 Jan  4 18:05 ._lb_plugins
-rwxr-xr-x  1 ubuntu ubuntu    163 Jan  5 12:38 ._lb_provisioner
-rwxr-xr-x  1 ubuntu ubuntu    163 Jan  3 23:34 ._lb_runner
-rwxr-xr-x  1 ubuntu ubuntu    163 Jan  7 22:40 ._lb_ui
-rw-r--r--  1 ubuntu ubuntu    219 Jan  9 21:34 ._pyproject.toml
-rw-r--r--  1 ubuntu ubuntu    163 Jan  9 21:44 ._uv.lock
drwxr-xr-x  3 ubuntu ubuntu   4096 Jan  9 21:35 .local
drwxr-xr-x  5 ubuntu ubuntu   4096 Jan  9 21:35 .venv
-rw-r--r--  1 ubuntu ubuntu   4230 Jan  3 23:34 README.md
drwxrwxr-x  2 ubuntu ubuntu   4096 Jan  9 21:35 data_exports
drwxr-xr-x  7 ubuntu ubuntu   4096 Jan  3 23:34 lb_analytics
drwxr-xr-x  4 ubuntu ubuntu   4096 Jan  3 23:34 lb_app
drwxr-xr-x  8 ubuntu ubuntu   4096 Jan  4 21:51 lb_common
drwxr-xr-x  8 ubuntu ubuntu   4096 Jan  3 23:34 lb_controller
drwxr-xr-x  5 ubuntu ubuntu   4096 Jan  4 18:05 lb_plugins
drwxr-xr-x  8 ubuntu ubuntu   4096 Jan  5 12:38 lb_provisioner
drwxr-xr-x  8 ubuntu ubuntu   4096 Jan  3 23:34 lb_runner
drwxr-xr-x 13 ubuntu ubuntu   4096 Jan  7 22:40 lb_ui
drwxr-xr-x  2 ubuntu ubuntu   4096 Jan  9 21:52 linux_benchmark_lib.egg-info
-rw-r--r--  1 ubuntu ubuntu   5472 Jan  9 21:34 pyproject.toml
drwxrwxr-x  2 ubuntu ubuntu   4096 Jan  9 21:35 reports
-rw-r--r--  1 ubuntu ubuntu 548226 Jan  9 21:44 uv.lock

=== LocalRunner status file ===
No status file

=== LocalRunner PID file ===
No PID file

=== benchmark_config.generated.json (first 200 lines) ===
No generated config

=== Event stream log ===
No stream log


--- LocalRunner Execution Evidence ---
=== Runner logs in /tmp ===
No runner logs found

=== Benchmark results ===

=== DFaaS-related files ===
/tmp/dfaas-k6-inventory.ini
/home/ubuntu/.lb/lb_plugins/plugins/._dfaas
/home/ubuntu/.lb/lb_plugins/plugins/dfaas/grafana/._dfaas-k6-dashboard.json
/home/ubuntu/.lb/lb_plugins/plugins/dfaas/grafana/dfaas-k6-dashboard.json
/home/ubuntu/.lb/lb_plugins/plugins/dfaas/grafana/dfaas-dashboard.json
/home/ubuntu/.lb/lb_plugins/plugins/dfaas/grafana/._dfaas-dashboard.json
/home/ubuntu/.ssh/dfaas_k6_key

=== Process history (if available) ===
/var/log/syslog

DFaaS-related files on target:
/tmp/dfaas-k6-inventory.ini
/tmp/dfaas-prometheus
/home/ubuntu/.lb/lb_plugins/plugins/._dfaas
/home/ubuntu/.lb/lb_plugins/plugins/dfaas
/home/ubuntu/.lb/lb_plugins/plugins/dfaas/grafana/._dfaas-k6-dashboard.json
/home/ubuntu/.lb/lb_plugins/plugins/dfaas/grafana/dfaas-k6-dashboard.json
/home/ubuntu/.lb/lb_plugins/plugins/dfaas/grafana/dfaas-dashboard.json
/home/ubuntu/.lb/lb_plugins/plugins/dfaas/grafana/._dfaas-dashboard.json
/home/ubuntu/.lb/lb_plugins/plugins/._dfaas
/home/ubuntu/.lb/lb_plugins/plugins/dfaas
/home/ubuntu/.lb/lb_plugins/plugins/dfaas/grafana/._dfaas-k6-dashboard.json
/home/ubuntu/.lb/lb_plugins/plugins/dfaas/grafana/dfaas-k6-dashboard.json
/home/ubuntu/.lb/lb_plugins/plugins/dfaas/grafana/dfaas-dashboard.json
/home/ubuntu/.lb/lb_plugins/plugins/dfaas/grafana/._dfaas-dashboard.json
/home/ubuntu/.ssh/dfaas_k6_key

Failed to find remote CSVs: Command '['multipass', 'exec', 'dfaas-target', '--', 'bash', '-c', "find /tmp -path '*benchmark_results*' -name '*.csv' 2>/dev/null"]' returned non-zero exit status 1.
============================================================

Verifying k6 logs on dfaas-generator in /home/ubuntu/.dfaas-k6
Found k6 logs on generator: []
No k6 logs found on generator dfaas-generator. Listing /home/ubuntu/.dfaas-k6 recursively with sudo:
Hostname:
dfaas-generator

Checking target VM dfaas-target for misplaced logs...
Found k6 logs on target: []
------------------------------ Captured log call -------------------------------
WARNING  tests.e2e.test_dfaas_multipass_e2e:test_dfaas_multipass_e2e.py:2397 DFaaS output directory not found locally - checking remote
WARNING  tests.e2e.test_dfaas_multipass_e2e:test_dfaas_multipass_e2e.py:1018 sudo find failed on dfaas-generator (multipass exec dfaas-generator -- sudo find /home/ubuntu/.dfaas-k6 -name k6.log -type f). stderr: find: â€˜/home/ubuntu/.dfaas-k6â€™: No such file or directory
WARNING  tests.e2e.test_dfaas_multipass_e2e:test_dfaas_multipass_e2e.py:1018 sudo find failed on dfaas-target (multipass exec dfaas-target -- sudo find /home/ubuntu/.dfaas-k6 -name k6.log -type f). stderr: find: â€˜/home/ubuntu/.dfaas-k6â€™: No such file or directory


                           Test Statistics by Marker                            
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“
â”ƒ Marker          â”ƒ Total â”ƒ Passed â”ƒ Failed â”ƒ Skipped â”ƒ Duration (s) â”ƒ Avg (s) â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©
â”‚ inter_e2e       â”‚     9 â”‚      5 â”‚      1 â”‚       3 â”‚       154.09 â”‚   17.12 â”‚
â”‚ inter_multipass â”‚     6 â”‚      2 â”‚      1 â”‚       3 â”‚       145.82 â”‚   24.30 â”‚
â”‚ inter_plugins   â”‚     6 â”‚      2 â”‚      1 â”‚       3 â”‚       145.82 â”‚   24.30 â”‚
â”‚ slow            â”‚     3 â”‚      3 â”‚      0 â”‚       0 â”‚         8.27 â”‚    2.76 â”‚
â”‚ slowest         â”‚     6 â”‚      2 â”‚      1 â”‚       3 â”‚       145.82 â”‚   24.30 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
=========================== short test summary info ============================
SKIPPED [1] tests/e2e/test_dfaas_multipass_e2e.py:101: setup_target playbook failed (context: setup_target)
SKIPPED [1] tests/e2e/test_dfaas_multipass_e2e.py:101: setup_target playbook failed (context: setup_target_streaming)
SKIPPED [1] tests/e2e/test_dfaas_multipass_e2e.py:101: setup_target failed (context: setup_target_events)
FAILED tests/e2e/test_dfaas_multipass_e2e.py::test_dfaas_multipass_cli_workflow
!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 1 failures !!!!!!!!!!!!!!!!!!!!!!!!!!!
